{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn #For neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_nn (nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_outputs):\n",
    "        super(custom_nn, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(n_inputs, n_hidden_neurons),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(n_hidden_neurons, n_outputs),\n",
    "                                   nn.Tanh()\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 10])\n",
      "torch.Size([15, 10])\n",
      "Wait - training the model\n",
      "Loss =  0.30525651574134827\n"
     ]
    }
   ],
   "source": [
    "ninputs = 10 #No of inputs\n",
    "nhidden = 5 #No of hidden neurons\n",
    "noutputs = 10 #No of outputs\n",
    "nexamples = 15 #No of examples to given to the neural network\n",
    "\n",
    "x = t.randn(nexamples, ninputs)\n",
    "y = x #Output is same as input - an autoencoder\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "nn1 = custom_nn(ninputs, nhidden, noutputs) #Create an object (nn1) of custom_nn class\n",
    "\n",
    "loss_fn = nn.MSELoss() #Loss function\n",
    "optimizer = t.optim.SGD(nn1.parameters(), lr = 0.01) #Defining the optimizer - SGD\n",
    "epochs = 100000\n",
    "\n",
    "print(\"Wait - training the model\")\n",
    "#Gradient Descent Algorithm\n",
    "for i in range(epochs):\n",
    "    ypred = nn1(x) #Calculate predicted value of the model\n",
    "    loss = loss_fn(ypred, y) #Apply the loss function (MSE) to calculate MSE\n",
    "    #print(\"Epoch : \", i+1, \"Loss = \", loss.item()) #Prints epoch number and its corresponding loss\n",
    "    loss.backward() #Backward - Backward propagation\n",
    "    optimizer.step() #Update all parameters\n",
    "    optimizer.zero_grad() #Set gradients of all parameters to zero before starting the next epoch\n",
    "    \n",
    "print(\"Loss = \", loss.item()) #Prints the final loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting output for unseen inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new =  tensor([[ 0.2847,  2.3132, -1.7834,  0.4504,  0.2247,  1.0256,  0.6928, -0.4136,\n",
      "          0.9663, -0.4992]])\n",
      "y_new =  tensor([[-0.9815,  1.0000, -0.9929,  0.9840, -0.9986, -0.3567,  0.7483, -0.9460,\n",
      "          0.5499,  0.9909]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_new = t.randn(1, ninputs) #New inputs\n",
    "y_new = nn1(x_new) #Predicting outputs using the trained model\n",
    "print(\"x_new = \", x_new)\n",
    "print(\"y_new = \", y_new) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
