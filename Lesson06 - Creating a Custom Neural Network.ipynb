{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn #For neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_nn (nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_outputs):\n",
    "        super(custom_nn, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(n_inputs, n_hidden_neurons),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(n_hidden_neurons, n_outputs),\n",
    "                                   nn.Tanh()\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 10])\n",
      "torch.Size([15, 10])\n",
      "Wait - training the model\n",
      "Epoch :  10000 Loss =  0.3378535211086273\n",
      "Epoch :  20000 Loss =  0.3123736083507538\n",
      "Epoch :  30000 Loss =  0.3037499189376831\n",
      "Epoch :  40000 Loss =  0.2975807785987854\n",
      "Epoch :  50000 Loss =  0.29111090302467346\n",
      "Epoch :  60000 Loss =  0.2869543433189392\n",
      "Epoch :  70000 Loss =  0.2834678292274475\n",
      "Epoch :  80000 Loss =  0.2808316648006439\n",
      "Epoch :  90000 Loss =  0.2790692150592804\n",
      "Epoch :  100000 Loss =  0.27769023180007935\n",
      "Loss =  0.27769023180007935\n",
      "CPU times: user 2min 37s, sys: 3.27 s, total: 2min 40s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ninputs = 10 #No of inputs\n",
    "nhidden = 5 #No of hidden neurons\n",
    "noutputs = 10 #No of outputs\n",
    "nexamples = 15 #No of examples to given to the neural network\n",
    "\n",
    "x = t.randn(nexamples, ninputs)\n",
    "y = x #Output is same as input - an autoencoder\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "  \n",
    "nn1 = custom_nn(ninputs, nhidden, noutputs) #Create an object (nn1) of custom_nn class\n",
    "\n",
    "loss_fn = nn.MSELoss() #Loss function\n",
    "optimizer = t.optim.SGD(nn1.parameters(), lr = 0.01) #Defining the optimizer - SGD\n",
    "epochs = 100000\n",
    "\n",
    "print(\"Wait - training the model\")\n",
    "#Gradient Descent Algorithm\n",
    "for i in range(epochs):\n",
    "    ypred = nn1(x) #Calculate predicted value of the model\n",
    "    loss = loss_fn(ypred, y) #Apply the loss function (MSE) to calculate MSE\n",
    "    if ((i+1) % 10000 == 0):\n",
    "        print(\"Epoch : \", i+1, \"Loss = \", loss.item()) #Prints loss after each 10,000 epochs\n",
    "    loss.backward() #Backward - Backward propagation\n",
    "    optimizer.step() #Update all parameters\n",
    "    optimizer.zero_grad() #Set gradients of all parameters to zero before starting the next epoch\n",
    "    \n",
    "print(\"Loss = \", loss.item()) #Prints the final loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting output for unseen inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new =  tensor([[-0.4099, -1.2812,  0.6133, -0.4557,  0.8936, -1.0523, -0.5323, -0.3767,\n",
      "         -0.7701, -0.3890]])\n",
      "y_new =  tensor([[ 1.0000, -0.9952,  0.7033,  0.9185,  0.9092, -0.9557,  1.0000,  0.1148,\n",
      "          0.9593, -0.0244]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_new = t.randn(1, ninputs) #New inputs\n",
    "y_new = nn1(x_new) #Predicting outputs using the trained model\n",
    "print(\"x_new = \", x_new)\n",
    "print(\"y_new = \", y_new) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network and data on a GPU if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU =  cuda:0\n",
      "torch.Size([15, 10])\n",
      "torch.Size([15, 10])\n",
      "Wait - training the model\n",
      "Epoch :  10000 Loss =  0.26431289315223694\n",
      "Epoch :  20000 Loss =  0.2288837730884552\n",
      "Epoch :  30000 Loss =  0.21779102087020874\n",
      "Epoch :  40000 Loss =  0.21157768368721008\n",
      "Epoch :  50000 Loss =  0.2073155641555786\n",
      "Epoch :  60000 Loss =  0.20453552901744843\n",
      "Epoch :  70000 Loss =  0.2024114578962326\n",
      "Epoch :  80000 Loss =  0.20072896778583527\n",
      "Epoch :  90000 Loss =  0.1993687003850937\n",
      "Epoch :  100000 Loss =  0.19824984669685364\n",
      "Loss =  0.19824984669685364\n",
      "CPU times: user 54.2 s, sys: 1.65 s, total: 55.8 s\n",
      "Wall time: 55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    device = t.device(\"cuda:0\") #If a GPU is availble, set device as first GPU (0 means first GPU)\n",
    "    print(\"Running on the GPU = \", device)\n",
    "else:\n",
    "    device = t.device(\"cpu\") #Else set the device as CPU\n",
    "    print(\"Running on the CPU = \", device)\n",
    "\n",
    "ninputs = 10 #No of inputs\n",
    "nhidden = 5 #No of hidden neurons\n",
    "noutputs = 10 #No of outputs\n",
    "nexamples = 15 #No of examples to given to the neural network\n",
    "\n",
    "x = t.randn(nexamples, ninputs, device=device)   #IMPORTANT - x is now on GPU if a GPU is available\n",
    "y = x #Output is same as input - an autoencoder  #IMPORTANT - y too is now on GPU if a GPU is available\n",
    "\n",
    "#IMPORTANT - be cautious if data is too large and if GPU memory is not fit enough for the data \n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "  \n",
    "nn1 = custom_nn(ninputs, nhidden, noutputs) #Create an object (nn1) of custom_nn class\n",
    "nn1 = nn1.to(device) #IMPORTANT - Neural network nn1 is passed to GPU if a GPU is available\n",
    "\n",
    "loss_fn = nn.MSELoss() #Loss function\n",
    "optimizer = t.optim.SGD(nn1.parameters(), lr = 0.01) #Defining the optimizer - SGD\n",
    "epochs = 100000\n",
    "\n",
    "print(\"Wait - training the model\")\n",
    "#Gradient Descent Algorithm\n",
    "for i in range(epochs):\n",
    "    ypred = nn1(x) #Calculate predicted value of the model\n",
    "    loss = loss_fn(ypred, y) #Apply the loss function (MSE) to calculate MSE\n",
    "    if ((i+1) % 10000 == 0):\n",
    "        print(\"Epoch : \", i+1, \"Loss = \", loss.item()) #Prints loss after each 10,000 epochs\n",
    "    loss.backward() #Backward - Backward propagation\n",
    "    optimizer.step() #Update all parameters\n",
    "    optimizer.zero_grad() #Set gradients of all parameters to zero before starting the next epoch\n",
    "    \n",
    "print(\"Loss = \", loss.item()) #Prints the final loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
